# LingCloud

The LingCloud 1.0 project seeks to enhance the large language model's capabilities by incorporating human-like eyes. 

I would like to express my sincere gratitude to Xinyu Chen, team member, and my advisor, Prof. [Baotian Hu](http://faculty.hitsz.edu.cn/hubaotian), for their tremendous support. 

Currently, GPT-4 has achieved unparalleled proficiency in image comprehension. However, given our limited computational resources and financial constraints, we must develop a model that can perform tasks akin to GPT-4. The aim is to connect visual information to the large language model (brain), thus increasing its ability to comprehend the external world's infinite-granularity visual content. This have been explored in previous works such as BLIP-2 and others. As a result, we present the first version of LingCloud, LingCloud 1.0, which will be continuously improved to achieve the robust and efficient interaction between LLMs and external world.


## Architecture

Here, you can see the detailed architecture and some experimental analyses of LingCloud 1.0.

If you have any question, please feel free to concat me by e-mail: liyunxin987@163.com, Twitter: [@LyxTg](https://twitter.com/LyxTg), or submit your issue in the repository.

## Presentation

some cases.

## How to run



## How to test




## Future Exploration

1. Finetune the LLMs with multimodal insturction data may decrease their performances on NLP. Could we jointly finetune LLMs with multimodal instruction data and text-only instruction-tuning data? How could we alleviate this bias?<br>
2. 
