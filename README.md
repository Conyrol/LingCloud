# LingCloud
This project (LingCloud 1.0) aims to attach human-like eyes to the large language model.

Many thanks to my team member (Xinyu Chen) and advisor (Prof. [Baotian Hu](http://faculty.hitsz.edu.cn/hubaotian)) for their great help.

Background: GPT-4's understanding of images has reached an unprecedented level. For us (without a lot of computing resources and financial support), how to design a model to achieve the capability similar to GPT-4? Is it possible to connect visual information to the large language model (brain) and increase its understanding of the external objective world (infinite-granularity visual content) understanding? Yes! such as BLIP-2 and others. So, based on pervious works, we present the project LingCloud and the first version LingCloud 1.0, which will be improved continuously. 

## Architecture

Here, you can see the detailed architecture and some experimental analyses of LingCloud 1.0.

If you have any question, please feel free to concat me by e-mail: liyunxin987@163.com, Twitter: @LyxTg, or submit your issue in the repository.

## Presentation

some cases.

## How to run?




## Future Exploration

1. Finetune the LLMs with multimodal insturction data may decrease their performances on NLP. Could we jointly finetune LLMs with multimodal instruction data and text-only instruction-tuning data? How can we alleviate this insturction-tuning bias?<br>
2. 
