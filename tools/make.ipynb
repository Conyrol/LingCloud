{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vcr1data:\n",
    "![vcr1data dataset info](./dataset_info/vcr1data.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'vcr-0', 'img_fn': 'lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'task_type': 'QA', 'question': 'Does person feel comfortable ? ', 'answer': 'No she does not . '}\n",
      "{'image_id': 'vcr-0', 'img_fn': 'lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'task_type': 'multi-choice', 'question': 'Does person feel comfortable ? ', 'answers': ['Yes because the person sitting next to her is smiling . ', 'No she does not . ', 'Yes , she is wearing something with thin straps . ', 'Yes , she is cold . '], 'answer_label': 1}\n",
      "212923\n",
      "212923\n",
      "['vcr1images/', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@2.json', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@2.jpg', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@9.jpg', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@0.json', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@9.json', 'vcr1images/movieclips_But_Who_Measured_the_First_Hill_SCENE_-_The_Englishman_Who_Went_Up_a_Hill_But_Came_Do..._MOVIE/fDA85m3v2TA@0.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/', 'vcr1images/movieclips_Play_It_Again_Sam/kF-KLIi97Uc@16.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/WdfgoDKArzM@8.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/kF-KLIi97Uc@16.json', 'vcr1images/movieclips_Play_It_Again_Sam/Z-Vnk_VTtzk@4.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/kF-KLIi97Uc@14.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/au5XE48PEeU@6.json', 'vcr1images/movieclips_Play_It_Again_Sam/au5XE48PEeU@8.json', 'vcr1images/movieclips_Play_It_Again_Sam/B8CGtuR9FnY@10.json', 'vcr1images/movieclips_Play_It_Again_Sam/B8CGtuR9FnY@10.jpg', 'vcr1images/movieclips_Play_It_Again_Sam/B8CGtuR9FnY@12.json', 'vcr1images/movieclips_Play_It_Again_Sam/8JkLimZnZAs@19.json']\n"
     ]
    }
   ],
   "source": [
    "# vcr1data\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/vcr1data_over/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/vcr_qa.json\"), \"r\") as f:\n",
    "    qa_file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/vcr_mutil-choice.json\"), \"r\") as f:\n",
    "    choice_file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "qa_json_data = json.loads(qa_file_content)\n",
    "choice_json_data = json.loads(choice_file_content)\n",
    "print(qa_json_data[0])\n",
    "print(choice_json_data[0])\n",
    "print(len(qa_json_data))\n",
    "print(len(choice_json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Does person feel comfortable ? ', 'answer': 'No she does not . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'Do person work together ? ', 'answer': 'Yes they do . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'Do person know each other ? ', 'answer': 'Yes they do . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'What is person feeling right now ? ', 'answer': 'He is feeling amused . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Why is person smiling at person ? ', 'answer': 'person has spinach in her teeth . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Where are person ? ', 'answer': 'They are at a restaurant . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Why is person laying down ? ', 'answer': 'They were pushed down . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'What is person doing ? ', 'answer': 'Cheering on the fighters . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Was person involved in the whole fight ? ', 'answer': 'No they were not . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'What is person about to do ? ', 'answer': 'They are opening up a note . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.21.22.023-02.21.30.292@2.jpg', 'chunk_belong': 'chunk7.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "qa_json_data_output = []\n",
    "\n",
    "for qa_data in qa_json_data:\n",
    "    query = qa_data['question']\n",
    "    answer = qa_data['answer']\n",
    "    image_path = 'vcr1images/' + qa_data['img_fn']\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    qa_json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_qa.json\"), 'w') as f:\n",
    "    f.write(json.dumps(qa_json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_qa.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Does person feel comfortable ? A. Yes because the person sitting next to her is smiling . B. No she does not . C. Yes , she is wearing something with thin straps . D. Yes , she is cold . ', 'answer': 'D. No she does not . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'Do person work together ? A. No they do not . B. Yes , person are walking together and are clearly dressed up in office wear . C. Yes they do . D. Yes , person are both security guards . ', 'answer': 'D. Yes they do . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'Do person know each other ? A. No . they do not know each other . B. No , person is taking person out of car . C. Yes they do . D. No , person are adversaries . ', 'answer': 'D. Yes they do . ', 'image_path': 'vcr1images/lsmdc_3015_CHARLIE_ST_CLOUD/3015_CHARLIE_ST_CLOUD_00.23.57.935-00.24.00.783@0.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'What is person feeling right now ? A. He is feeling despair . B. He is confused and can not believe what he is hearing person tell him . C. He is feeling pretty scared . D. He is feeling amused . ', 'answer': 'D. He is feeling amused . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Why is person smiling at person ? A. person has given her some spare change . B. She is happy to see him happy . C. person has spinach in her teeth . D. person complimented how she looks . ', 'answer': 'D. person has spinach in her teeth . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Where are person ? A. person are in a basement . B. They are in a workshop . C. person are in the airport . D. They are at a restaurant . ', 'answer': 'D. They are at a restaurant . ', 'image_path': 'vcr1images/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Why is person laying down ? A. person has fallen ill . B. They were pushed down . C. person is relaxing after a long day . D. person is being interrogated by person . ', 'answer': 'D. They were pushed down . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'What is person doing ? A. person is buying something . B. Cheering on the fighters . C. person is leaving the room after talking with person . D. person is doing a ballroom dance with person . ', 'answer': 'D. Cheering on the fighters . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Was person involved in the whole fight ? A. Yes , it appears that person may die in this fight . B. No person is betting on the opponent . C. No they were not . D. No , person did not start the fire . ', 'answer': 'D. No they were not . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.10.26.133-02.10.30.224@0.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'What is person about to do ? A. person is about to play hockey . B. person is about to go inside the building . C. They are opening up a note . D. person is about to wake person . ', 'answer': 'D. They are opening up a note . ', 'image_path': 'vcr1images/lsmdc_1051_Harry_Potter_and_the_goblet_of_fire/1051_Harry_Potter_and_the_goblet_of_fire_02.21.22.023-02.21.30.292@2.jpg', 'chunk_belong': 'chunk7.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# choice data\n",
    "option_list = ['A. ', 'B. ', 'C. ', 'D. ', 'E. ', 'F. ', 'G. ', 'H. ']\n",
    "choice_json_data_output = []\n",
    "\n",
    "for choice_data in choice_json_data:\n",
    "    query = choice_data['question']\n",
    "    answers = choice_data['answers']\n",
    "    answer_index = choice_data['answer_label']\n",
    "    for index, label in enumerate(answers):\n",
    "        query += option_list[index] + label\n",
    "    answer = option_list[index] + answers[answer_index]\n",
    "\n",
    "    image_path = 'vcr1images/' + choice_data['img_fn']\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    choice_json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_choice.json\"), 'w') as f:\n",
    "    f.write(json.dumps(choice_json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_choice.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scienceqa:\n",
    "![scienceqa dataset info](./dataset_info/Scienceqa.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'science-1', 'task_type': 'scienceQA', 'question': 'Which of these states is farthest north?', 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'], 'answer': 0, 'lecture': 'Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\\nA compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\\nThe north arrow points to the North Pole. On most maps, north is at the top of the map.'}\n",
      "6218\n",
      "['train/', 'train/21045/', 'train/21045/image.png', 'train/4535/', 'train/4535/image.png', 'train/18285/', 'train/18285/choice_0.png', 'train/18285/choice_1.png', 'train/12930/', 'train/12930/image.png', 'train/1576/', 'train/1576/image.png', 'train/6566/', 'train/6566/choice_0.png', 'train/6566/choice_1.png', 'train/6566/image.png', 'train/12019/', 'train/12019/image.png', 'train/6649/', 'train/6649/image.png']\n"
     ]
    }
   ],
   "source": [
    "# scienceqa\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/scienceqa/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/scienceqa_scienceqa.json\"), \"r\") as f:\n",
    "    choice_file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "choice_json_data = json.loads(choice_file_content)\n",
    "\n",
    "print(choice_json_data[0])\n",
    "print(len(choice_json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which of these states is farthest north? A. West Virginia B. Louisiana C. Arizona D. Oklahoma', 'answer': ' D. West Virginia', 'image_path': 'train/1/image.png', 'chunk_belong': 'chunk1.zip', 'lecture': 'Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\\nA compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\\nThe north arrow points to the North Pole. On most maps, north is at the top of the map.'}\n"
     ]
    }
   ],
   "source": [
    "# choice data\n",
    "option_list = [' A. ', ' B. ', ' C. ', ' D. ', ' E. ', ' F. ', ' G. ', ' H. ']\n",
    "choice_json_data_output = []\n",
    "\n",
    "for choice_data in choice_json_data:\n",
    "    query = choice_data['question']\n",
    "    answers = choice_data['choices']\n",
    "    answer_index = choice_data['answer']\n",
    "    for index, label in enumerate(answers):\n",
    "        query += option_list[index] + label\n",
    "    answer = option_list[index] + answers[answer_index]\n",
    "    lecture = choice_data['lecture']\n",
    "\n",
    "    image_path = 'train/' + choice_data['image_id'].replace('science-', '') + '/image.png'\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    choice_json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "        \"lecture\": lecture,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_choice.json\"), 'w') as f:\n",
    "    f.write(json.dumps(choice_json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data_choice.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minigpt4_qa:\n",
    "![minigpt4_qa dataset info](./dataset_info/minigpt4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'minigpt4-2', 'data_type': 'QA', 'question': ' Who or what is shown in the image? ', 'answer': ' The image shows a man fishing on a lawn next to a river with a bridge in the background.'}\n",
      "17195\n",
      "['image/', 'image/3006.jpg', 'image/1920.jpg', 'image/2893.jpg', 'image/4698.jpg', 'image/4917.jpg', 'image/1992.jpg', 'image/201.jpg', 'image/268.jpg', 'image/3809.jpg', 'image/349.jpg', 'image/1373.jpg', 'image/3498.jpg', 'image/1694.jpg', 'image/4956.jpg', 'image/4528.jpg', 'image/4908.jpg', 'image/1487.jpg', 'image/3284.jpg', 'image/4532.jpg']\n"
     ]
    }
   ],
   "source": [
    "# minigpt4_qa\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/minigpt4/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/minigpt4_qa.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': ' Who or what is shown in the image? ', 'answer': ' The image shows a man fishing on a lawn next to a river with a bridge in the background.', 'image_path': 'image/2.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What can be seen on the other side of the river? ', 'answer': ' Trees can be seen on the other side of the river.', 'image_path': 'image/2.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What is the weather like in the image? ', 'answer': ' The sky is cloudy in the image.', 'image_path': 'image/2.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What can be seen in the background of the image? ', 'answer': ' A bridge can be seen in the background of the image.', 'image_path': 'image/2.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' Where is the man fishing in the image? ', 'answer': ' The man is fishing on a lawn next to a river in the image.', 'image_path': 'image/2.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What material are the countertops made of in the kitchen?', 'answer': ' The countertops in the kitchen are made of black granite.', 'image_path': 'image/3.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What kind of appliances are there in the kitchen?', 'answer': ' The kitchen has stainless steel appliances, including a refrigerator, oven, and dishwasher.', 'image_path': 'image/3.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What is the color of the backsplash behind the stove in the kitchen?', 'answer': ' The backsplash behind the stove in the kitchen is white in color.', 'image_path': 'image/3.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' What is the color of the walls in the kitchen?', 'answer': ' The walls in the kitchen are painted white.', 'image_path': 'image/3.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': ' Is there a door in the kitchen that leads to the outside?', 'answer': ' Yes, there is a door in the kitchen that leads to the outside.', 'image_path': 'image/3.jpg', 'chunk_belong': 'chunk1.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = 'image/' + '{}.jpg'.format(data['image_id'].replace('minigpt4-', ''))\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flickr30K\n",
    "![flickr30K dataset info](./dataset_info/Flickr30K.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/flickr30K/images/flickr30k-images/1000092795.jpg', 'question': 'Write a short description for the image.', 'answer': 'two young guys with shaggy hair look at their hands while hanging out in the yard .'}\n",
      "158915\n",
      "['flickr30K/images/flickr30k-images/', 'flickr30K/images/flickr30k-images/3187364311.jpg', 'flickr30K/images/flickr30k-images/6894721598.jpg', 'flickr30K/images/flickr30k-images/1199847613.jpg', 'flickr30K/images/flickr30k-images/4865202740.jpg', 'flickr30K/images/flickr30k-images/2609836649.jpg', 'flickr30K/images/flickr30k-images/2134207062.jpg', 'flickr30K/images/flickr30k-images/2626553824.jpg', 'flickr30K/images/flickr30k-images/1809758121.jpg', 'flickr30K/images/flickr30k-images/1251583490.jpg', 'flickr30K/images/flickr30k-images/3550860765.jpg', 'flickr30K/images/flickr30k-images/715855028.jpg', 'flickr30K/images/flickr30k-images/4541692312.jpg', 'flickr30K/images/flickr30k-images/3587781729.jpg', 'flickr30K/images/flickr30k-images/1348891916.jpg', 'flickr30K/images/flickr30k-images/3690431163.jpg', 'flickr30K/images/flickr30k-images/4631831983.jpg', 'flickr30K/images/flickr30k-images/2873445888.jpg', 'flickr30K/images/flickr30k-images/7302659678.jpg', 'flickr30K/images/flickr30k-images/1299459550.jpg', 'flickr30K/images/flickr30k-images/4696106624.jpg', 'flickr30K/images/flickr30k-images/435966316.jpg', 'flickr30K/images/flickr30k-images/105493603.jpg', 'flickr30K/images/flickr30k-images/6878449442.jpg', 'flickr30K/images/flickr30k-images/2970067128.jpg', 'flickr30K/images/flickr30k-images/4932786099.jpg', 'flickr30K/images/flickr30k-images/4646506674.jpg', 'flickr30K/images/flickr30k-images/5683548982.jpg', 'flickr30K/images/flickr30k-images/567921691.jpg', 'flickr30K/images/flickr30k-images/382143413.jpg', 'flickr30K/images/flickr30k-images/4501347788.jpg', 'flickr30K/images/flickr30k-images/4940544962.jpg', 'flickr30K/images/flickr30k-images/111201721.jpg', 'flickr30K/images/flickr30k-images/3284460070.jpg', 'flickr30K/images/flickr30k-images/8194331212.jpg', 'flickr30K/images/flickr30k-images/5350403659.jpg', 'flickr30K/images/flickr30k-images/4824252063.jpg', 'flickr30K/images/flickr30k-images/3085226474.jpg', 'flickr30K/images/flickr30k-images/6911538978.jpg', 'flickr30K/images/flickr30k-images/363792435.jpg', 'flickr30K/images/flickr30k-images/1347001782.jpg', 'flickr30K/images/flickr30k-images/6978881720.jpg', 'flickr30K/images/flickr30k-images/6858310184.jpg', 'flickr30K/images/flickr30k-images/4571040008.jpg', 'flickr30K/images/flickr30k-images/4146483752.jpg', 'flickr30K/images/flickr30k-images/4524361920.jpg', 'flickr30K/images/flickr30k-images/2047030603.jpg', 'flickr30K/images/flickr30k-images/3111402233.jpg', 'flickr30K/images/flickr30k-images/3050490720.jpg', 'flickr30K/images/flickr30k-images/4326902979.jpg', 'flickr30K/images/flickr30k-images/4772306571.jpg', 'flickr30K/images/flickr30k-images/3285993030.jpg', 'flickr30K/images/flickr30k-images/4725672154.jpg', 'flickr30K/images/flickr30k-images/5222736851.jpg', 'flickr30K/images/flickr30k-images/281271503.jpg', 'flickr30K/images/flickr30k-images/3604314527.jpg', 'flickr30K/images/flickr30k-images/3258472448.jpg', 'flickr30K/images/flickr30k-images/3375534917.jpg', 'flickr30K/images/flickr30k-images/256698336.jpg', 'flickr30K/images/flickr30k-images/5921627737.jpg', 'flickr30K/images/flickr30k-images/4257307377.jpg', 'flickr30K/images/flickr30k-images/2326133103.jpg', 'flickr30K/images/flickr30k-images/1250745375.jpg', 'flickr30K/images/flickr30k-images/2565703445.jpg', 'flickr30K/images/flickr30k-images/2993049054.jpg', 'flickr30K/images/flickr30k-images/2863042793.jpg', 'flickr30K/images/flickr30k-images/2444134813.jpg', 'flickr30K/images/flickr30k-images/229189301.jpg', 'flickr30K/images/flickr30k-images/1935532481.jpg', 'flickr30K/images/flickr30k-images/2332986053.jpg', 'flickr30K/images/flickr30k-images/177522874.jpg', 'flickr30K/images/flickr30k-images/3103645451.jpg', 'flickr30K/images/flickr30k-images/8142543165.jpg', 'flickr30K/images/flickr30k-images/3673661352.jpg', 'flickr30K/images/flickr30k-images/2846785268.jpg', 'flickr30K/images/flickr30k-images/2359100647.jpg', 'flickr30K/images/flickr30k-images/4320882376.jpg', 'flickr30K/images/flickr30k-images/41911141.jpg', 'flickr30K/images/flickr30k-images/383051932.jpg', 'flickr30K/images/flickr30k-images/5505219003.jpg', 'flickr30K/images/flickr30k-images/2841196844.jpg', 'flickr30K/images/flickr30k-images/2149521096.jpg', 'flickr30K/images/flickr30k-images/4739671194.jpg', 'flickr30K/images/flickr30k-images/458833944.jpg', 'flickr30K/images/flickr30k-images/2335634931.jpg', 'flickr30K/images/flickr30k-images/6885133284.jpg', 'flickr30K/images/flickr30k-images/741227062.jpg', 'flickr30K/images/flickr30k-images/4935946626.jpg', 'flickr30K/images/flickr30k-images/1066252238.jpg', 'flickr30K/images/flickr30k-images/4823839290.jpg', 'flickr30K/images/flickr30k-images/1079274291.jpg', 'flickr30K/images/flickr30k-images/3879927955.jpg', 'flickr30K/images/flickr30k-images/2545363449.jpg', 'flickr30K/images/flickr30k-images/2993318965.jpg', 'flickr30K/images/flickr30k-images/4588947862.jpg', 'flickr30K/images/flickr30k-images/3042173467.jpg', 'flickr30K/images/flickr30k-images/3522076584.jpg', 'flickr30K/images/flickr30k-images/1450988692.jpg', 'flickr30K/images/flickr30k-images/4827261484.jpg', 'flickr30K/images/flickr30k-images/2666681650.jpg']\n"
     ]
    }
   ],
   "source": [
    "# flickr30K\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/flickr30K/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/FLICKR.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk3.zip\"))\n",
    "print(zip_file_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Write a short description for the image.', 'answer': 'two young guys with shaggy hair look at their hands while hanging out in the yard .', 'image_path': 'flickr30K/images/flickr30k-images/1000092795.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'two young , white males are outside near many bushes .', 'image_path': 'flickr30K/images/flickr30k-images/1000092795.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'two men in green shirts are standing in a yard .', 'image_path': 'flickr30K/images/flickr30k-images/1000092795.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'A photo of:', 'answer': 'a man in a blue shirt standing in a garden .', 'image_path': 'flickr30K/images/flickr30k-images/1000092795.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'An image that shows', 'answer': 'two friends enjoy time spent together .', 'image_path': 'flickr30K/images/flickr30k-images/1000092795.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Write a short description for the image.', 'answer': 'several men in hard hats are operating a giant pulley system .', 'image_path': 'flickr30K/images/flickr30k-images/10002456.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'Please provide a short depiction of the picture.', 'answer': 'workers look down from up above on a piece of equipment .', 'image_path': 'flickr30K/images/flickr30k-images/10002456.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'Please provide a short depiction of the picture.', 'answer': 'two men working on a machine wearing hard hats .', 'image_path': 'flickr30K/images/flickr30k-images/10002456.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'An image that shows', 'answer': 'four men on top of a tall structure .', 'image_path': 'flickr30K/images/flickr30k-images/10002456.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'Briefly describe the content of the image.', 'answer': 'three men on a large rig .', 'image_path': 'flickr30K/images/flickr30k-images/10002456.jpg', 'chunk_belong': 'chunk3.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iconqa\n",
    "![iconqa dataset info](./dataset_info/iconqa.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/iconqa/iconqa_data/iconqa/train/choose_txt/70172/image.png', 'question': 'The first picture is a boat. Which picture is sixth?\\nA.bucket\\nB.crab\\nC.boat', 'answer': 'A.bucket'}\n",
      "18946\n",
      "['iconqa/', 'iconqa/iconqa_data/', 'iconqa/iconqa_data/pid_splits.json', 'iconqa/iconqa_data/pid2skills.json', 'iconqa/iconqa_data/iconqa/', 'iconqa/iconqa_data/iconqa/train/', 'iconqa/iconqa_data/iconqa/train/choose_txt/', 'iconqa/iconqa_data/iconqa/train/choose_txt/70172/', 'iconqa/iconqa_data/iconqa/train/choose_txt/70172/data.json', 'iconqa/iconqa_data/iconqa/train/choose_txt/70172/image.png', 'iconqa/iconqa_data/iconqa/train/choose_txt/57421/', 'iconqa/iconqa_data/iconqa/train/choose_txt/57421/data.json', 'iconqa/iconqa_data/iconqa/train/choose_txt/57421/image.png', 'iconqa/iconqa_data/iconqa/train/choose_txt/94579/', 'iconqa/iconqa_data/iconqa/train/choose_txt/94579/data.json', 'iconqa/iconqa_data/iconqa/train/choose_txt/94579/image.png', 'iconqa/iconqa_data/iconqa/train/choose_txt/48621/', 'iconqa/iconqa_data/iconqa/train/choose_txt/48621/data.json', 'iconqa/iconqa_data/iconqa/train/choose_txt/48621/image.png', 'iconqa/iconqa_data/iconqa/train/choose_txt/28921/']\n"
     ]
    }
   ],
   "source": [
    "# iconqa\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/iconqa/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/ICONQA.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'The first picture is a boat. Which picture is sixth?\\nA.bucket\\nB.crab\\nC.boat', 'answer': 'A.bucket', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/70172/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'The first picture is a rocket. Which picture is fifth?\\nA.moon\\nB.star\\nC.rocket', 'answer': 'B.star', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/57421/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'What has been done to this letter?\\nA.turn\\nB.slide\\nC.flip', 'answer': 'A.turn', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/94579/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'What fraction of the shapes are circles?\\nA.2/7\\nB.8/12\\nC.3/11\\nD.2/9', 'answer': 'C.3/11', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/48621/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'The first picture is a bucket. Which picture is ninth?\\nA.crab\\nB.boat\\nC.bucket', 'answer': 'A.crab', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/28921/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'What has been done to this letter?\\nA.slide\\nB.turn\\nC.flip', 'answer': 'C.flip', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/101259/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'Are there enough acorns for every squirrel?\\nA.yes\\nB.no', 'answer': 'B.no', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/15219/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'The first picture is a bucket. Which picture is fourth?\\nA.bucket\\nB.crab\\nC.boat', 'answer': 'B.crab', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/36244/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'How many stars are there?\\nA.9\\nB.6\\nC.1\\nD.8\\nE.10', 'answer': 'E.10', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/105159/image.png', 'chunk_belong': 'chunk1.zip'}, {'query': 'Is the number of hearts even or odd?\\nA.odd\\nB.even', 'answer': 'B.even', 'image_path': 'iconqa/iconqa_data/iconqa/train/choose_txt/7773/image.png', 'chunk_belong': 'chunk1.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRV-Instruction\n",
    "![LRV-Instruction dataset info](./dataset_info/LRV-Instruction.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/LRV-Instruction/VG_100K/2365199.jpg', 'question': 'Does the buckle on the purse have a size of Width:50 Height:50?', 'answer': 'The buckle on the purse is described as having Width:19 Height:19, not Width:50 Height:50.'}\n",
      "139186\n",
      "['LRV-Instruction/', 'LRV-Instruction/LRV_Instruction_train.json', 'LRV-Instruction/LRV-Instruction.json', 'LRV-Instruction/LRV_Instruction_val.json', 'LRV-Instruction/VG_100K/', 'LRV-Instruction/VG_100K/2357866.jpg', 'LRV-Instruction/VG_100K/2334264.jpg', 'LRV-Instruction/VG_100K/2375107.jpg', 'LRV-Instruction/VG_100K/2321384.jpg', 'LRV-Instruction/VG_100K/2410870.jpg', 'LRV-Instruction/VG_100K/2328233.jpg', 'LRV-Instruction/VG_100K/2326313.jpg', 'LRV-Instruction/VG_100K/2362602.jpg', 'LRV-Instruction/VG_100K/2368855.jpg', 'LRV-Instruction/VG_100K/2412960.jpg', 'LRV-Instruction/VG_100K/2319117.jpg', 'LRV-Instruction/VG_100K/2398946.jpg', 'LRV-Instruction/VG_100K/2321836.jpg', 'LRV-Instruction/VG_100K/2376100.jpg', 'LRV-Instruction/VG_100K/2384204.jpg']\n"
     ]
    }
   ],
   "source": [
    "# LRV-Instruction\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/LRV-Instruction/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/LRV.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Does the buckle on the purse have a size of Width:50 Height:50?', 'answer': 'The buckle on the purse is described as having Width:19 Height:19, not Width:50 Height:50.', 'image_path': 'LRV-Instruction/VG_100K/2365199.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'The bunch of ripe, yellow bananas is on the right side of the image.', 'answer': 'There are unripe, green bananas in the image, not ripe, yellow bananas.', 'image_path': 'LRV-Instruction/VG_100K/2408352.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Can you spot a lighthouse near the waves in the water?', 'answer': 'This instruction is misleading because there is no mention of a lighthouse in the image.', 'image_path': 'LRV-Instruction/VG_100K/2410690.jpg', 'chunk_belong': 'chunk5.zip'}, {'query': 'What kind of area surrounds the snowboarder in the image?', 'answer': 'A large wooded area with leafless trees', 'image_path': 'LRV-Instruction/VG_100K/2352698.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': \"Describe the hair dryer's location.\", 'answer': 'On the bathroom wall', 'image_path': 'LRV-Instruction/VG_100K/2394487.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': \"Check if there's a man with a red scarf at X:95 Y:49?\", 'answer': 'The object at this position is \"a lady with a scarf around her neck,\" not a man with a red scarf.', 'image_path': 'LRV-Instruction/VG_100K/2409537.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'List the main objects found in the image.', 'answer': 'Red boat, blue and white boat, motor, lights, ripples, dock, building, pole, windows, railing, ferry', 'image_path': 'LRV-Instruction/VG_100K/2359867.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'Notice the vibrant orange leaves on the trees in the background.', 'answer': 'The trees are described as green, not with vibrant orange leaves.', 'image_path': 'LRV-Instruction/VG_100K/2408090.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Is the lady holding the ski poles wearing goggles as well?', 'answer': 'No, she is not.', 'image_path': 'LRV-Instruction/VG_100K/2366751.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Is the fork lying on top of the chicken parmesan?', 'answer': 'The fork has coordinates X:4 Y:88 and dimensions Width:70 Height:70, while the chicken parmesan is located at X:234 Y:43 with dimensions Width:264 Height:264. The fork and the chicken parmesan are not overlapping, so the fork is not lying on top of the dish.', 'image_path': 'LRV-Instruction/VG_100K/2384516.jpg', 'chunk_belong': 'chunk5.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VisualDialog\n",
    "![VisualDialog dataset info](./dataset_info/VisualDialog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/VisualDialog/train2014/COCO_train2014_000000378466.jpg', 'question': 'is this a child or adult', 'answer': 'adult'}\n",
      "123287\n",
      "['VisualDialog/', 'VisualDialog/VisualDialog_val2018/', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000532309.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000005363.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000372889.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000500009.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000269405.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000559418.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000238149.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000556619.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000214432.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000313886.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000179778.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000458949.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000023817.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000322431.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000504128.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000366462.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000534775.jpg', 'VisualDialog/VisualDialog_val2018/VisualDialog_val2018_000000479069.jpg']\n"
     ]
    }
   ],
   "source": [
    "# VisualDialog\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/VisualDialog/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/VisualDialog.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'is this a child or adult', 'answer': 'adult', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000378466.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'what color is horse', 'answer': \"brown, but it's black and white photo\", 'image_path': 'VisualDialog/train2014/COCO_train2014_000000575029.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'how many bikes there', 'answer': '3', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000287140.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'what color is the sink', 'answer': 'white', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000378461.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'is this a zoo', 'answer': 'yes', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000332243.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'is it a baby elephant', 'answer': \"no, it's a very large adult\", 'image_path': 'VisualDialog/train2014/COCO_train2014_000000012597.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'are the beds for children', 'answer': 'no', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000054318.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'is the person a man or woman', 'answer': \"can't tell they are wearing a white paper suit\", 'image_path': 'VisualDialog/train2014/COCO_train2014_000000370250.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'is the image of an ocean', 'answer': 'yes', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000011544.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'are the elephants both male', 'answer': 'not sure', 'image_path': 'VisualDialog/train2014/COCO_train2014_000000370252.jpg', 'chunk_belong': 'chunk4.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vsr\n",
    "![vsr dataset info](./dataset_info/vsr.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/vsr/images/images/000000239417.jpg', 'question': 'Could you use a few words to describe what you perceive in the photo?', 'answer': 'The bench is at the right side of the train.'}\n",
      "10972\n",
      "['vsr/', 'vsr/images/', 'vsr/images/train2017/', 'vsr/images/train2017/000000449088.jpg', 'vsr/images/train2017/000000387048.jpg', 'vsr/images/train2017/000000061389.jpg', 'vsr/images/train2017/000000172733.jpg', 'vsr/images/train2017/000000084114.jpg', 'vsr/images/train2017/000000506379.jpg', 'vsr/images/train2017/000000165562.jpg', 'vsr/images/train2017/000000016573.jpg', 'vsr/images/train2017/000000335434.jpg', 'vsr/images/train2017/000000131579.jpg', 'vsr/images/train2017/000000579589.jpg', 'vsr/images/train2017/000000352482.jpg', 'vsr/images/train2017/000000435742.jpg', 'vsr/images/train2017/000000511654.jpg', 'vsr/images/train2017/000000537982.jpg', 'vsr/images/train2017/000000442554.jpg', 'vsr/images/train2017/000000440340.jpg']\n"
     ]
    }
   ],
   "source": [
    "# vsr\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/vsr/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/VSR.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Could you use a few words to describe what you perceive in the photo?', 'answer': 'The bench is at the right side of the train.', 'image_path': 'vsr/images/images/000000239417.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'The train contains the laptop.', 'image_path': 'vsr/images/images/000000224670.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'The bowl is at the left side of the banana.', 'image_path': 'vsr/images/images/000000266622.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'A short image description:', 'answer': 'The cat is under the motorcycle.', 'image_path': 'vsr/images/images/000000546264.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Write a short description for the image.', 'answer': 'The bicycle is near the tv.', 'image_path': 'vsr/images/images/000000490624.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'A photo of:', 'answer': 'The person is ahead of the cow.', 'image_path': 'vsr/images/images/000000080336.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Could you use a few words to describe what you perceive in the photo?', 'answer': 'The hair drier is in front of the person.', 'image_path': 'vsr/images/images/000000563615.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Provide a description of what is presented in the photo.', 'answer': 'The giraffe is on the bench.', 'image_path': 'vsr/images/images/000000194528.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'A short image description:', 'answer': 'The bed is below the sheep.', 'image_path': 'vsr/images/images/000000557254.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Briefly describe the content of the image.', 'answer': 'The person is touching the handbag.', 'image_path': 'vsr/images/images/000000383211.jpg', 'chunk_belong': 'chunk7.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### okvqa\n",
    "![okvqa dataset info](./dataset_info/okvqa.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/okvqa/images/train/train2014/COCO_train2014_000000051606.jpg', 'question': 'What is the hairstyle of the blond called?', 'answer': 'pony tail'}\n",
      "9009\n",
      "['okvqa/', 'okvqa/annotations/', 'okvqa/annotations/mscoco_train2014_annotations.json', 'okvqa/annotations/mscoco_val2014_annotations.json', 'okvqa/images/', 'okvqa/images/train/', 'okvqa/images/train/train2014/', 'okvqa/images/train/train2014/COCO_train2014_000000209563.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000169656.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000136672.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000338704.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000560384.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000225787.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000057676.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000190645.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000237188.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000563651.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000186056.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000382929.jpg', 'okvqa/images/train/train2014/COCO_train2014_000000574722.jpg']\n"
     ]
    }
   ],
   "source": [
    "# okvqa\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/okvqa/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/OKVQA.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'What is the hairstyle of the blond called?', 'answer': 'pony tail', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000051606.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'How old do you have to be in canada to do this?', 'answer': '18', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000081721.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Can you guess the place where the man is playing?', 'answer': 'aspen', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000480208.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'Which rail company is named after a town in new mexico?', 'answer': 'santa fe', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000570618.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Is the boy swimming or doing another water activity?', 'answer': 'another activity', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000478903.jpg', 'chunk_belong': 'chunk3.zip'}, {'query': 'Why do kids stare?', 'answer': 'curious', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000542651.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'When was this piece of sporting equipment invented?', 'answer': '1926', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000025024.jpg', 'chunk_belong': 'chunk1.zip'}, {'query': 'Why is this officer using this mode of transportation?', 'answer': 'patrol', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000002056.jpg', 'chunk_belong': 'chunk2.zip'}, {'query': 'Why is this an atypical look for someone wearing this makeup?', 'answer': 'clown', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000179526.jpg', 'chunk_belong': 'chunk4.zip'}, {'query': 'Which american president is most associated with the stuffed animal seen here?', 'answer': 'theodore roosevelt', 'image_path': 'okvqa/images/train/train2014/COCO_train2014_000000386927.jpg', 'chunk_belong': 'chunk2.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQAv2\n",
    "![VQAv2 dataset info](./dataset_info/VQAv2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/vqav2/images/train2014/COCO_train2014_000000458752.jpg', 'question': 'What is this photo taken looking through?', 'answer': 'net'}\n",
      "443757\n",
      "['vqav2/', 'vqav2/annotations/', 'vqav2/annotations/v2_mscoco_val2014_annotations.json', 'vqav2/annotations/.swo', 'vqav2/annotations/.swp', 'vqav2/annotations/v2_mscoco_train2014_annotations.json', 'vqav2/images/', 'vqav2/images/test2015/', 'vqav2/images/test2015/COCO_test2015_000000552743.jpg', 'vqav2/images/test2015/COCO_test2015_000000250438.jpg', 'vqav2/images/test2015/COCO_test2015_000000381374.jpg', 'vqav2/images/test2015/COCO_test2015_000000141782.jpg', 'vqav2/images/test2015/COCO_test2015_000000193746.jpg', 'vqav2/images/test2015/COCO_test2015_000000529564.jpg', 'vqav2/images/test2015/COCO_test2015_000000156198.jpg', 'vqav2/images/test2015/COCO_test2015_000000540649.jpg', 'vqav2/images/test2015/COCO_test2015_000000198452.jpg', 'vqav2/images/test2015/COCO_test2015_000000392836.jpg', 'vqav2/images/test2015/COCO_test2015_000000469076.jpg', 'vqav2/images/test2015/COCO_test2015_000000237400.jpg']\n"
     ]
    }
   ],
   "source": [
    "# VQAv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/vqav2/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/VQAv2.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'What is this photo taken looking through?', 'answer': 'net', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000458752.jpg', 'chunk_belong': 'chunk9.zip'}, {'query': 'What position is this man playing?', 'answer': 'pitcher', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000458752.jpg', 'chunk_belong': 'chunk9.zip'}, {'query': 'What color is the players shirt?', 'answer': 'orange', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000458752.jpg', 'chunk_belong': 'chunk9.zip'}, {'query': 'Is this man a professional baseball player?', 'answer': 'yes', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000458752.jpg', 'chunk_belong': 'chunk9.zip'}, {'query': 'What color is the snow?', 'answer': 'white', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000262146.jpg', 'chunk_belong': 'chunk8.zip'}, {'query': 'What is the person doing?', 'answer': 'skiing', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000262146.jpg', 'chunk_belong': 'chunk8.zip'}, {'query': 'What color is the persons headwear?', 'answer': 'red', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000262146.jpg', 'chunk_belong': 'chunk8.zip'}, {'query': \"What is in the person's hand?\", 'answer': 'frisbee', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000524291.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Is the dog waiting?', 'answer': 'yes', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000524291.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Is the dog looking at a tennis ball or frisbee?', 'answer': 'frisbee', 'image_path': 'vqav2/images/train2014/COCO_train2014_000000524291.jpg', 'chunk_belong': 'chunk7.zip'}]\n"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCOCap\n",
    "![COCOCap dataset info](./dataset_info/COCOCap.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000318556.jpg', 'question': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'A very clean and well decorated empty bathroom'}\n",
      "414113\n",
      "['coco/', 'coco/captions_val2014.json', 'coco/captions_train2014.json', 'coco/images/', 'coco/images/test2015/', 'coco/images/test2015/COCO_test2015_000000552743.jpg', 'coco/images/test2015/COCO_test2015_000000250438.jpg', 'coco/images/test2015/COCO_test2015_000000381374.jpg', 'coco/images/test2015/COCO_test2015_000000141782.jpg', 'coco/images/test2015/COCO_test2015_000000193746.jpg', 'coco/images/test2015/COCO_test2015_000000529564.jpg', 'coco/images/test2015/COCO_test2015_000000156198.jpg', 'coco/images/test2015/COCO_test2015_000000540649.jpg', 'coco/images/test2015/COCO_test2015_000000198452.jpg', 'coco/images/test2015/COCO_test2015_000000392836.jpg', 'coco/images/test2015/COCO_test2015_000000469076.jpg', 'coco/images/test2015/COCO_test2015_000000237400.jpg', 'coco/images/test2015/COCO_test2015_000000555203.jpg', 'coco/images/test2015/COCO_test2015_000000074375.jpg', 'coco/images/test2015/COCO_test2015_000000479209.jpg']\n"
     ]
    }
   ],
   "source": [
    "# TEXTCAPS\n",
    "import os\n",
    "import json\n",
    "\n",
    "root_path = \"/root/data/mutimodel_dataset/data_split/coco/\"\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/COCOCap.json\"), \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "with open(os.path.join(root_path, \"raw_json/chunk_belong.json\"), \"r\") as f:\n",
    "    chunk_belong = f.read()\n",
    "\n",
    "chunk_belong_dict = json.loads(chunk_belong)\n",
    "json_data = json.loads(file_content)\n",
    "\n",
    "print(json_data[0])\n",
    "print(len(json_data))\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "zip_file_list = list_files_in_zip(os.path.join(root_path, \"image/chunk1.zip\"))\n",
    "print(zip_file_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'A very clean and well decorated empty bathroom', 'image_path': 'coco/images/train2014/COCO_train2014_000000318556.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'A panoramic view of a kitchen and all of its appliances.', 'image_path': 'coco/images/train2014/COCO_train2014_000000116100.jpg', 'chunk_belong': 'chunk8.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'A blue and white bathroom with butterfly themed wall tiles.', 'image_path': 'coco/images/train2014/COCO_train2014_000000318556.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Provide a description of what is presented in the photo.', 'answer': 'A panoramic photo of a kitchen and dining room', 'image_path': 'coco/images/train2014/COCO_train2014_000000116100.jpg', 'chunk_belong': 'chunk8.zip'}, {'query': 'Write a short description for the image.', 'answer': 'A graffiti-ed stop sign across the street from a red car ', 'image_path': 'coco/images/train2014/COCO_train2014_000000379340.jpg', 'chunk_belong': 'chunk6.zip'}, {'query': 'Could you use a few words to describe what you perceive in the photo?', 'answer': 'A vandalized stop sign and a red beetle on the road', 'image_path': 'coco/images/train2014/COCO_train2014_000000379340.jpg', 'chunk_belong': 'chunk6.zip'}, {'query': 'Using language, provide a short account of the image.', 'answer': 'A bathroom with a border of butterflies and blue paint on the walls above it.', 'image_path': 'coco/images/train2014/COCO_train2014_000000318556.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Provide a description of what is presented in the photo.', 'answer': 'An angled view of a beautifully decorated bathroom.', 'image_path': 'coco/images/train2014/COCO_train2014_000000318556.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'The two people are walking down the beach.', 'image_path': 'coco/images/train2014/COCO_train2014_000000134754.jpg', 'chunk_belong': 'chunk7.zip'}, {'query': 'Please provide a short depiction of the picture.', 'answer': 'A sink and a toilet inside a small bathroom.', 'image_path': 'coco/images/train2014/COCO_train2014_000000538480.jpg', 'chunk_belong': 'chunk9.zip'}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# qa data\n",
    "json_data_output = []\n",
    "\n",
    "for data in json_data:\n",
    "    query = data['question']\n",
    "    answer = data['answer']\n",
    "    image_path = data['image_path'].replace('Datasets/', '')\n",
    "    chunk_belong = chunk_belong_dict[image_path]\n",
    "    \n",
    "    json_data_output.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"image_path\": image_path,\n",
    "        \"chunk_belong\": chunk_belong,\n",
    "    })\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'w') as f:\n",
    "    f.write(json.dumps(json_data_output))\n",
    "\n",
    "with open(os.path.join(root_path, \"json/data.json\"), 'r') as f:\n",
    "    print(json.loads(f.read())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000318556.jpg', 'question': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'A very clean and well decorated empty bathroom'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000116100.jpg', 'question': 'Using language, provide a short account of the image.', 'answer': 'A panoramic view of a kitchen and all of its appliances.'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000318556.jpg', 'question': 'Using language, provide a short account of the image.', 'answer': 'A blue and white bathroom with butterfly themed wall tiles.'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000116100.jpg', 'question': 'Provide a description of what is presented in the photo.', 'answer': 'A panoramic photo of a kitchen and dining room'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000379340.jpg', 'question': 'Write a short description for the image.', 'answer': 'A graffiti-ed stop sign across the street from a red car '}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000379340.jpg', 'question': 'Could you use a few words to describe what you perceive in the photo?', 'answer': 'A vandalized stop sign and a red beetle on the road'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000318556.jpg', 'question': 'Using language, provide a short account of the image.', 'answer': 'A bathroom with a border of butterflies and blue paint on the walls above it.'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000318556.jpg', 'question': 'Provide a description of what is presented in the photo.', 'answer': 'An angled view of a beautifully decorated bathroom.'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000134754.jpg', 'question': 'Use a few words to illustrate what is happening in the picture.', 'answer': 'The two people are walking down the beach.'}, {'image_path': 'Datasets/coco/images/train2014/COCO_train2014_000000538480.jpg', 'question': 'Please provide a short depiction of the picture.', 'answer': 'A sink and a toilet inside a small bathroom.'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/root/data/mutimodel_dataset/json/COCOCap.json\", \"r\") as f:\n",
    "    qa_file_content = f.read()\n",
    "qa_json_data = json.loads(qa_file_content)\n",
    "print(qa_json_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coco/', 'coco/captions_val2014.json', 'coco/captions_train2014.json', 'coco/images/', 'coco/images/test2015/', 'coco/images/test2015/COCO_test2015_000000552743.jpg', 'coco/images/test2015/COCO_test2015_000000250438.jpg', 'coco/images/test2015/COCO_test2015_000000381374.jpg', 'coco/images/test2015/COCO_test2015_000000141782.jpg', 'coco/images/test2015/COCO_test2015_000000193746.jpg']\n"
     ]
    }
   ],
   "source": [
    "def list_files_in_zip(zip_file_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        file_list = zip_file.namelist()\n",
    "    return file_list\n",
    "\n",
    "print(list_files_in_zip(\"/root/data/mutimodel_dataset/image/coco.zip\")[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
