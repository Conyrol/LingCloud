# LingCloud

The LingCloud project seeks to enhance the large language model's capabilities by incorporating human-like eyes. 

I would like to express my sincere gratitude to Xinyu Chen, work member, and my advisor, Prof. [Baotian Hu](http://faculty.hitsz.edu.cn/hubaotian), for their tremendous support. 

Currently, GPT-4 has achieved unparalleled proficiency in image comprehension. Given our limited computational resources and financial supports, we also need to develop a model that can perform various tasks akin to GPT-4. The aim of this project is to connect visual information to the large language model (brain), thus increasing its ability to comprehend the external world's infinite-granularity visual content. As a result, we present the first version of LingCloud, LingCloud 1.0, which will be continuously improved to achieve the robust and efficient interaction between LLMs and the external world.


## Architecture

Here, you can see the detailed architecture and some experimental analyses of LingCloud 1.0.


![](https://github.com/YunxinLi/LingCloud/blob/main/images/model.png)



If you have any question, please feel free to contact me by e-mail: liyunxin987@163.com, Twitter: [@LyxTg](https://twitter.com/LyxTg), or submit your issue in the repository.

## Presentation

Demo will come soon.

We present some cases as follows.


## How to run



## How to test




## Future Exploration

1. Finetune the LLMs with multimodal insturction data may decrease their performances on NLP. In this paper, we find that text instruction-following tuning LLMs have better generalization on performing multimodal interaction.
For future, could we jointly finetune LLMs with multimodal instruction data and text-only instruction-tuning data? How could we alleviate this bias?<br>
2. 


## Acknowledge
